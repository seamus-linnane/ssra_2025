{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526c8dbb",
   "metadata": {},
   "source": [
    "# üß™ CNN Classifier for Waveform Image Data\n",
    "\n",
    "In this notebook, you will build and fine-tune a **Convolutional Neural Network (CNN)** to classify medical waveform images ‚Äî such as plethysmography or ECG lead II traces.\n",
    "\n",
    "We use a **pretrained ResNet18** model and customize it to learn from a small dataset representing different classes:  \n",
    "e.g.\n",
    "- `as`      (Traces from patients with Aortic Stenosis)  \n",
    "- `control` (Traces from control patients)  \n",
    "\n",
    "This project introduces key machine learning concepts:\n",
    "- Preparing image data for training\n",
    "- Using file naming conventions for auto-labeling\n",
    "- Creating a robust train/val/test folder structure\n",
    "- Fine-tuning a CNN using transfer learning\n",
    "- Tracking model performance with accuracy, loss, and confusion matrices\n",
    "\n",
    "> üéØ **Goal**: Understand how a CNN can learn from medical image patterns ‚Äî even in small datasets ‚Äî and evaluate model performance with appropriate metrics.\n",
    "\n",
    "You can experiment by:\n",
    "- Adding your own image classes\n",
    "- Modifying batch size, learning rate, or architecture\n",
    "- Inspecting misclassified examples\n",
    "\n",
    "Record any changes you make and the effect on the accuracy of your model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9539d9",
   "metadata": {},
   "source": [
    "## üß† A note on CNNs\n",
    "\n",
    "A **Convolutional Neural Network** is a type of neural network designed to automatically learn spatial patterns in images. It applies filters (like edge detectors or curve recognizers) at multiple layers and learns hierarchical features ‚Äî from pixels to shapes to complex patterns.\n",
    "\n",
    "## üèóÔ∏è Our Model: ResNet18\n",
    "\n",
    "We use a **pretrained ResNet18 model**, which was originally trained on **ImageNet** ‚Äî a massive dataset of **over 1 million images** across **1,000 classes** (e.g., cats, planes, trees, tools, etc.).\n",
    "\n",
    "ResNet18 uses a deep architecture with **residual blocks**, which help it learn efficiently even when the network is deep (18 layers). This architecture has become a standard in medical imaging research.\n",
    "\n",
    "## üõ†Ô∏è What We're Doing Here\n",
    "\n",
    "We keep the powerful pretrained layers ‚Äî which already know how to detect general visual features (edges, curves, textures) ‚Äî and **only modify the final classification layer** to output **2 classes** instead of 1,000.\n",
    "\n",
    "This approach is called **transfer learning**, specifically **fine-tuning**:\n",
    "- We first freeze the pretrained layers and only train the final layer on our waveform data.\n",
    "- Then we \"unfreeze\" the model and continue training the whole network gently to adjust to our task.\n",
    "\n",
    "We are modifing a general purpose image recognizer into a specialized waveform classifier ‚Äî with limited clinical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210bbd3",
   "metadata": {},
   "source": [
    "# üß≠ Getting Started: Imports and Path Setup\n",
    "\n",
    "This section loads all the Python libraries you'll need ‚Äî including `torch`, `torchvision`, and `sklearn`.\n",
    "\n",
    "We also define the base folders for your project. Make sure:\n",
    "- Your waveform images are placed inside the `input/` folder.\n",
    "- Each image filename begins with its class eg `as_image001.jpg`.  \n",
    "- The `output/` folder is where your preprocessed image files will be saved in training, validation, and test subfolders.\n",
    "- Each subfolder will have further subfolders automatically named for each class.  \n",
    "- The `output/` folder is also where the trained model weights will be saved.  \n",
    "\n",
    "‚ö†Ô∏è Make sure `base_path` points to the root of your project on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6249746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "base_path = Path.cwd()\n",
    "input_folder = base_path / \"input\"\n",
    "output_folder = base_path / \"output\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826f15a",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è Understanding Your Data \n",
    "\n",
    "We scan the `input/` folder and list the available files.  \n",
    "We create a list of files in the variable `image_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
    "\n",
    "image_files = []\n",
    "for f in input_folder.glob(\"*\"):\n",
    "    try:\n",
    "        if f.suffix.lower() in image_extensions and f.is_file():\n",
    "            image_files.append(f)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped {f.name}: {e}\")\n",
    "\n",
    "print(f\"Found {len(image_files)} images:\")\n",
    "for file in image_files[:10]:  # Only print first 10\n",
    "    print(f\"File: {file.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00869268",
   "metadata": {},
   "source": [
    "# üß© Map Class Names & Integer Labels\n",
    "\n",
    "In this step, we list the class names we'll be working with (e.g. `as`, `control`) and automatically assign each one a unique integer label starting from 0.\n",
    "We create a dictionary called `class_dict` that maps each class name.  \n",
    "This mapping is important because neural networks require numerical labels ‚Äî not text ‚Äî for classification.\n",
    "\n",
    "üí° For example:\n",
    "- `\"as\"` ‚Üí `0`\n",
    "- `\"control\"` ‚Üí `1`\n",
    "\n",
    "These will be used during model training and when building your folder structure.  \n",
    "You will also refer to this map at inference - when you run your trained model - to decode the model output back into clinically recognisable classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063de1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = [\"as\", \"control\"]  # You may need to change classes based on your project\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# üìã Prepare key and value lists\n",
    "class_keys = []\n",
    "class_values = []\n",
    "\n",
    "# üî¢ Manually assign variables like class_key0 = \"as\", class_value0 = 0, etc.\n",
    "for i in range(len(class_names)):\n",
    "    key_var = f\"class_key{i}\"\n",
    "    value_var = f\"class_value{i}\"\n",
    "\n",
    "    key = class_names[i]\n",
    "    value = i\n",
    "\n",
    "    class_keys.append(key)\n",
    "    class_values.append(value)\n",
    "\n",
    "class_dict = dict(zip(class_keys, class_values))\n",
    "\n",
    "# üñ®Ô∏è Preview the result\n",
    "print(\"class_dict =\", class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063012bb",
   "metadata": {},
   "source": [
    "# Organize Image Files by Class\n",
    "\n",
    "We iterate through the `image_files` list and assign each image to its class based on filename prefix (e.g. `as_001.png` ‚Üí `\"as\"`).  \n",
    "We create a dictionary called `class_files` that holds a list of image files for each class, grouped by their filename prefix.  \n",
    "We will later split these into training, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fdd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üóÉÔ∏è Create a dictionary to hold files for each class\n",
    "class_files = {class_key: [] for class_key in class_keys}\n",
    "\n",
    "# üîÅ Loop through all images and assign to the correct class list\n",
    "for file in image_files: # We created this list at the very beginning.\n",
    "    for class_key in class_keys:\n",
    "        if file.name.lower().startswith(class_key):\n",
    "            class_files[class_key].append(file)\n",
    "            break  # Stop after first match\n",
    "\n",
    "# üñ®Ô∏è Summary\n",
    "for class_key, files in class_files.items():\n",
    "    print(f\"\\nüìÅ {class_key.upper()} ({len(files)} files):\")\n",
    "    for f in files[:3]:  # show first 3 files per class\n",
    "        print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7dca39",
   "metadata": {},
   "source": [
    "# üßº Preprocess, Resize, and Split Data into Train/Val/Test\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "- **Standardize image size and format**  \n",
    "   We define a function called `standardise_image()` that loads each image, converts it to RGB, and resizes it to 224√ó224 pixels.  \n",
    "   This is the format expected by ResNet18.\n",
    "\n",
    "- **Create the output subfolders required by our classes.**  \n",
    "    The CNN expects train, test and validate folders.  \n",
    "    We check the number and names of the classes.  \n",
    "    We create child folders named for each class in each of the train, test and validate folders\n",
    "\n",
    "- **Split into train/val/test and save**  \n",
    "   We split the files 80% for training, 10% for validation, and 10% for testing.  \n",
    "   We save the standardised images to a structured `output/` directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d14566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standardise_image(path, size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Open an image, convert to RGB, resize to target size using PIL.\n",
    "    \"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = img.resize(size)\n",
    "    return img\n",
    "\n",
    "\n",
    "# üóÇÔ∏è Define where to place the processed files\n",
    "output_folder = output_folder\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "split_ratios = (0.8, 0.1, 0.1)  # 8:1:1\n",
    "\n",
    "# üìÇ Preview and create the required folder structure\n",
    "print(\"üì¶ Creating output folder structure:\")\n",
    "for split in splits:\n",
    "    for cls in class_keys:\n",
    "        dest_dir = output_folder / split / cls\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"  {Path(split) / cls}\")\n",
    "\n",
    "# üîÄ Split and copy the files for each class\n",
    "for cls in class_keys:\n",
    "    files = class_files[cls]\n",
    "    \n",
    "    # Train/val/test split using sklearn\n",
    "    train_files, temp_files = train_test_split(files, test_size=(1 - split_ratios[0]), random_state=42)\n",
    "    val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n",
    "\n",
    "    split_map = {\n",
    "        \"train\": train_files,\n",
    "        \"val\": val_files,\n",
    "        \"test\": test_files\n",
    "    }\n",
    "\n",
    "    # üì• Copy each file into its split/class folder\n",
    "    for split, split_files in split_map.items():\n",
    "        for src_path in split_files:\n",
    "            dst_path = output_folder / split / cls / src_path.name\n",
    "            standardise_image(src_path).save(dst_path)\n",
    "\n",
    "# ‚úÖ Done\n",
    "print(\"\\n‚úÖ Images successfully preprocessed and saved into clild folders in `output/` for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9cac06",
   "metadata": {},
   "source": [
    "# üß† Model Setup: ResNet18 for Classification\n",
    "\n",
    "We use a pre-trained ResNet18 model ‚Äî a deep convolutional network that has already learned to recognize general features like edges, textures, and shapes.\n",
    "\n",
    "We'll:\n",
    "- Freeze the base layers\n",
    "- Replace the final classification layer\n",
    "- Train it on our waveform images (e.g., plethysmography or ECG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0659917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# üîß HYPERPARAMETERS\n",
    "# ---------------------------\n",
    "\n",
    "num_classes = num_classes\n",
    "\n",
    "train_batch_size = 8\n",
    "val_batch_size = 32\n",
    "initial_lr = 0.0001\n",
    "momentum = 0.9\n",
    "frozen_epochs = 6\n",
    "finetune_epochs = 20\n",
    "\n",
    "train_path = output_folder / \"train\"\n",
    "val_path = output_folder / \"val\"\n",
    "ft_model_path = output_folder\n",
    "model_weights_filename = 'model_weights.pth'\n",
    "\n",
    "# ---------------------------\n",
    "# üñºÔ∏è TRANSFORM\n",
    "# ---------------------------\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# üìÅ DATASETS & LOADERS\n",
    "# ---------------------------\n",
    "\n",
    "train_dataset = ImageFolder(train_path, transform=transform)\n",
    "val_dataset = ImageFolder(val_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "print(\"üß≠ Class to index mapping:\", train_dataset.class_to_idx)\n",
    "\n",
    "# ---------------------------\n",
    "# üß† MODEL INIT\n",
    "# ---------------------------\n",
    "\n",
    "model = models.resnet18(weights='ResNet18_Weights.IMAGENET1K_V1')\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available()\n",
    "                      else \"cuda\" if torch.cuda.is_available()\n",
    "                      else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=initial_lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed3e540",
   "metadata": {},
   "source": [
    "# üîÅ Model Training\n",
    "\n",
    "We train the model in **two phases**:\n",
    "1. **Frozen base** ‚Äî only the final layer is trained for a few epochs.\n",
    "2. **Fine-tuning** ‚Äî all layers are unfrozen and trained further to adapt the model more fully to our dataset.\n",
    "\n",
    "Training and validation losses and accuracies are printed per epoch.\n",
    "\n",
    "üìà This allows us to track whether the model is overfitting (memorizing training data) or generalizing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_corrects.float() / len(train_dataset)\n",
    "\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "        val_loss = running_loss / len(val_dataset)\n",
    "        val_acc = running_corrects.float() / len(val_dataset)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc.item())\n",
    "        val_accuracies.append(val_acc.item())\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] ‚Üí \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2%} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2%}\")\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "\n",
    "\n",
    "# Freeze phase\n",
    "metrics1 = train(model, train_loader, val_loader, criterion, optimizer, num_epochs=frozen_epochs)\n",
    "\n",
    "# Unfreeze and retrain\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum)\n",
    "metrics2 = train(model, train_loader, val_loader, criterion, optimizer, num_epochs=finetune_epochs)\n",
    "\n",
    "print('so far so good')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d90fc8",
   "metadata": {},
   "source": [
    "# üìâ Visualizing Training Progress\n",
    "\n",
    "Here we plot:\n",
    "- Training vs Validation **Loss** (how wrong the model was)\n",
    "- Training vs Validation **Accuracy** (how often it was right)\n",
    "\n",
    "These plots help you understand:\n",
    "- Is your model improving?\n",
    "- Is it overfitting?\n",
    "- Did it converge?\n",
    "\n",
    "üö® Flat lines or diverging curves may signal problems with your data, learning rate, or model capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, title=\"Training Curves\"):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # üìâ Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # üìà Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Acc\")\n",
    "    plt.plot(epochs, val_accuracies, label=\"Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Merge and plot\n",
    "train_losses = metrics1[0] + metrics2[0]\n",
    "val_losses = metrics1[1] + metrics2[1]\n",
    "train_accuracies = metrics1[2] + metrics2[2]\n",
    "val_accuracies = metrics1[3] + metrics2[3]\n",
    "\n",
    "plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839bb95e",
   "metadata": {},
   "source": [
    "# ‚úÖ Evaluating Performance\n",
    "\n",
    "We compute a **confusion matrix** and calculate:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "\n",
    "These metrics give a complete picture of how well your model is classifying the different classes, especially when the dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# üîç Evaluate on validation set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# üìä Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "class_labels = train_dataset.classes  # ‚Üê from ImageFolder, usually alphabetical\n",
    "\n",
    "# üé® Plot heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix (Validation Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7638a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_metrics_from_cm(cm):\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, recall, and F1 score from a 2x2 confusion matrix.\n",
    "    Assumes binary classification with cm = [[TP, FN], [FP, TN]] format.\n",
    "    \"\"\"\n",
    "    if cm.shape != (2, 2):\n",
    "        raise ValueError(\"This function only supports binary classification (2x2 confusion matrix).\")\n",
    "\n",
    "    tp = cm[0, 0]\n",
    "    fn = cm[0, 1]\n",
    "    fp = cm[1, 0]\n",
    "    tn = cm[1, 1]\n",
    "\n",
    "    total = tp + fn + fp + tn\n",
    "\n",
    "    accuracy = (tp + tn) / total if total else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }\n",
    "\n",
    "metrics_from_cm = compute_binary_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\nüìã Evaluation Metrics from Confusion Matrix:\")\n",
    "for metric, value in metrics_from_cm.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b18ef",
   "metadata": {},
   "source": [
    "# üéâ Inference of the new finetuned model\n",
    "\n",
    "Now that we‚Äôve trained our CNN model, it's time to put it to the test!\n",
    "\n",
    "Earlier, we carefully set aside a **test group** during preprocessing that the model has **never seen**. This allows us to evaluate how well our model generalizes to new data.\n",
    "\n",
    "Below, we‚Äôll run inference on each test image and compare the **true class** to the **predicted class** using our fine-tuned ResNet-18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# üîÅ Use the same transform as training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# üìÇ Load test data\n",
    "test_path = output_folder / \"test\"\n",
    "test_dataset = ImageFolder(test_path, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # batch_size=1 for per-file printing\n",
    "\n",
    "# üè∑Ô∏è Map class index to label\n",
    "idx_to_class = {v: k for k, v in test_dataset.class_to_idx.items()}\n",
    "\n",
    "# üîç Run inference and show per-file results\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"\\nüñºÔ∏è Predictions on Test Files:\")\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        true_label = idx_to_class[labels.item()]\n",
    "        pred_label = idx_to_class[preds.item()]\n",
    "\n",
    "        # Get image filename\n",
    "        img_path = test_dataset.samples[i][0]\n",
    "        filename = Path(img_path).name\n",
    "\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{filename}\\n  true class = {true_label:<10}   predicted class = {pred_label:<10}\")\n",
    "\n",
    "        all_preds.append(preds.item())\n",
    "        all_labels.append(labels.item())\n",
    "\n",
    "# üìä Summary\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\n‚úÖ Test Accuracy: {test_acc:.2%}\")\n",
    "print(\"\\nüìÑ Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=test_dataset.classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
